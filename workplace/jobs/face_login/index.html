<script>
    function isWebGLSupported() {
        try {
            const canvas = document.createElement('canvas');
            return !!window.WebGLRenderingContext && (canvas.getContext('webgl') || canvas.getContext('experimental-webgl'));
        } catch (e) {
            return false;
        }
    }

    if (isWebGLSupported()) {
        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('./models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('./models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('./models'),
            faceapi.nets.ssdMobilenetv1.loadFromUri('./models')
        ]).then(() => {
            Swal.close();
            console.log('load model success');
            $('i.bi-camera-video').addClass('d-none');
            $('#video').removeClass('d-none');
            openWebcamForSelfie();
        }).catch((err) => console.log(err));
    } else {
        Swal.fire({
            icon: 'error',
            title: 'WebGL Not Supported',
            html: 'Your device does not support WebGL, which is required for this application. Please enable WebGL or use the <code>--enable-unsafe-swiftshader</code> flag for trusted content.',
        });
    }

    async function openWebcamForSelfie() {
        const video = document.getElementById('video');
        navigator.mediaDevices
            .getUserMedia({
                video: {
                    facingMode: "user",
                    width: $('#video').width(),
                    height: $('#video').height()
                },
                audio: false,
            })
            .then((stream) => {
                video.srcObject = stream;
            })
            .catch((error) => {
                console.error(error);
            });
    }

    $('#video').on('loadeddata', async function () {
        const storedDescription = localStorage.getItem('faceDescription');
        if (storedDescription) {
            const parsedDescription = JSON.parse(storedDescription);
            const labeledFaceDescriptors = new faceapi.LabeledFaceDescriptors("123456", [new Float32Array(parsedDescription)]);
            console.log("Loaded face description from localStorage:", labeledFaceDescriptors);
            matchFace(labeledFaceDescriptors);
        }
        setTimeout(async () => {
            const video = document.getElementById('video');
            const displaySize = {
                width: $('#video').width(),
                height: $('#video').height()
            };
            faceapi.matchDimensions(video, displaySize);

            const canvas = faceapi.createCanvasFromMedia(video);
            const context = canvas.getContext('2d', { willReadFrequently: true });
            setInterval(async () => {
                const labels = ["123456"]; // Add your own label
                const descriptions = [];
                const detections = await faceapi
                    .detectSingleFace(canvas, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptor();
                console.log("ðŸš€ !! detections:", detections)
                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                // document.getElementById('camera').append(canvas);
                faceapi.draw.drawDetections(canvas, resizedDetections);
                // faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                descriptions.push(detections.descriptor);
                localStorage.setItem('faceDescription', JSON.stringify(descriptions[0]));
                console.log(new faceapi.LabeledFaceDescriptors(labels[0], descriptions));
                matchFace(new faceapi.LabeledFaceDescriptors(labels[0], descriptions));

            }, 100);
        }, 200);
    });


    async function matchFace(labeledFaceDescriptors) {
        const video = document.getElementById('video');
        const displaySize = {
            width: $('#video').width(),
            height: $('#video').height()
        };
        faceapi.matchDimensions(video, displaySize);

        const canvas = faceapi.createCanvasFromMedia(video);
        const context = canvas.getContext('2d', { willReadFrequently: true });
        // setInterval(async () => {
        const labels = ["123456"]; // Add your own label
        const descriptions = [];
        const detections = await faceapi
            .detectSingleFace(canvas, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptor();
        console.log("ðŸš€ !! detections:", detections)
        const resizedDetections = faceapi.resizeResults(detections, displaySize);
        document.getElementById('camera').append(canvas);
        faceapi.draw.drawDetections(canvas, resizedDetections);
        // faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
        descriptions.push(detections.descriptor);
        console.log(new faceapi.LabeledFaceDescriptors(labels[0], descriptions));
        const faceMatcher = new faceapi.FaceMatcher(new faceapi.LabeledFaceDescriptors(labels[0], descriptions));
        const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor));
        console.log("ðŸš€ !! results:", results)
    }
</script>